theme: jekyll-theme-cayman
# Evaluaci-n-big-data
Repositorio para la evaluaci√≥n 2 big data
# Evaluaci√≥n Parcial 2 - Big Data
# Evaluaci√≥n Parcial 2 - Big Data

**Nombres Antonella de los √Ångeles Ugalde Bruna  Valentina Jorquera
**Asignatura:** AVY1101 - Big Data  
**Evaluaci√≥n:** Evaluaci√≥n Parcial N¬∞2  
**Modalidad:** Batch ‚Äì Proyecto Big Data  
**Repositorio:** Sitio de Proyecto GitHub  
**Fecha de entrega:** [Agrega la fecha aqu√≠]

---

## üìå √çndice

1. [Introducci√≥n](#1-introducci√≥n)  
2. [Conexi√≥n y obtenci√≥n de datos](#2-conexi√≥n-y-obtenci√≥n-de-datos)  
3. [DataLake y almacenamiento](#3-datalake-y-almacenamiento)  
4. [Limpieza y transformaci√≥n](#4-limpieza-y-transformaci√≥n)  
5. [Validaci√≥n, errores y control de duplicados](#5-validaci√≥n-errores-y-control-de-duplicados)  
6. [Visualizaci√≥n y an√°lisis (Dashboard)](#6-visualizaci√≥n-y-an√°lisis-dashboard)  
7. [Conclusiones](#7-conclusiones)  
8. [Anexos](#8-anexos)

---

## 1. Introducci√≥n

El presente proyecto corresponde a la Evaluaci√≥n Parcial N¬∞2 de la asignatura de Big Data, y tiene como objetivo aplicar los conocimientos sobre procesos Batch para la ingesta, transformaci√≥n, almacenamiento y an√°lisis de grandes vol√∫menes de datos en un entorno de Big Data.

Se aborda una carga hist√≥rica de datos desde diversas fuentes, considerando criterios de validaci√≥n, control de errores y visualizaci√≥n de insights relevantes mediante dashboards interactivos. Este proceso permitir√° responder preguntas clave del negocio y simular un flujo completo de procesamiento de datos en la nube.

---

## 2. Conexi√≥n y obtenci√≥n de datos

Para este proyecto se trabaj√≥ con datos obtenidos desde [nombre fuente, por ejemplo: archivos CSV p√∫blicos / Kaggle / bases simuladas].

Los archivos fueron descargados desde:  
- [URL del dataset o fuente de datos]  
- En formato: `.csv`, `.json`, [otro]  

La obtenci√≥n se automatiz√≥ mediante scripts en Python utilizando bibliotecas como `pandas`, `requests` o conectores espec√≠ficos, y fueron guardados inicialmente en el entorno local / GCS.

---

## 3. DataLake y almacenamiento

Se utiliz√≥ un enfoque de almacenamiento en un DataLake basado en [elige uno: Google Cloud Storage / BigQuery / Amazon S3].

Estructura del almacenamiento:

DataLake/
‚îÇ
‚îú‚îÄ‚îÄ raw/ # Datos originales
‚îú‚îÄ‚îÄ processed/ # Datos limpios y transformados
‚îî‚îÄ‚îÄ analytics/ # Tablas para an√°lisis y visualizaci√≥n

Los datos fueron cargados en el DataLake usando comandos SQL o scripts automatizados con `bq`, `gsutil` o herramientas de ETL como [indica cu√°l usaste].

---

## 4. Limpieza y transformaci√≥n

Los datos presentaban diversos problemas que fueron abordados en esta etapa:

- Eliminaci√≥n de filas con valores nulos
- Conversi√≥n de tipos de datos (fechas, n√∫meros)
- Normalizaci√≥n de nombres y formatos
- Eliminaci√≥n de duplicados
- Creaci√≥n de nuevas columnas para an√°lisis
- Agregaci√≥n por categor√≠as o fechas

La transformaci√≥n se implement√≥ en Python con `pandas` y luego se cargaron en una tabla final para an√°lisis en BigQuery.

---

## 5. Validaci√≥n, errores y control de duplicados

Se incorporaron controles para asegurar la calidad y estabilidad del flujo:

- Validaci√≥n de estructura del archivo (columnas obligatorias)
- Control de duplicados: verificaci√≥n con hashes o claves √∫nicas
- Control de ejecuci√≥n: registro de procesos realizados (log)
- Manejo de errores: try/except y alertas en consola/logs
- Reprocesamiento hist√≥rico: opci√≥n de volver a ejecutar por fecha

---

## 6. Visualizaci√≥n y an√°lisis (Dashboard)

Se construyeron 4 gr√°ficos representativos utilizando [Python / Power BI / Tableau]:

1. üìä Distribuci√≥n por categor√≠a  
2. üìà Evoluci√≥n temporal  
3. üß≠ Comparaci√≥n entre grupos  
4. üìå An√°lisis predictivo b√°sico (opcional)

![grafico1](visualizations/grafico1.png)  
![grafico2](visualizations/grafico2.png)

Los gr√°ficos permiten extraer insights clave que pueden apoyar la toma de decisiones del √°rea de negocio.

---

## 7. Conclusiones

El desarrollo de este proyecto permiti√≥ simular un flujo completo de procesamiento Batch en un entorno Big Data, comprendiendo desde la ingesta hasta la entrega de visualizaciones anal√≠ticas. Se lograron identificar errores comunes, automatizar procesos y estructurar un flujo robusto, √∫til para la operaci√≥n de datos a gran escala.

Adem√°s, se reforzaron habilidades en limpieza, transformaci√≥n y uso de herramientas en la nube como GCP, junto con la capacidad de comunicar resultados visualmente.

---

## 8. Anexos

- Script de descarga de datos: `scripts/descarga_datos.py`
- Script de limpieza: `scripts/limpieza.py`
- SQL para carga en BigQuery: `scripts/carga_bq.sql`
- Gr√°ficos: carpeta `/visualizations/`
- Diagrama del flujo de proceso: [opcional: puedes subir una imagen]

---
